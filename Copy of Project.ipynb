{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39852,"status":"ok","timestamp":1701802021833,"user":{"displayName":"Robert D'Antonio","userId":"11761168232811440342"},"user_tz":300},"id":"0q4OUMbxPmUI","outputId":"7feb2307-927a-4936-9c81-339ec9170ba4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["'''\n","Mount Google Drive, copy data to runtime, and unzip folders\n","\n","Make sure to put a link to \"EC 523 Project\" in your main google drive!\n","'''\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","! cp /content/drive/'My Drive'/'EC 523 Project'/CompressedData/train.zip /content\n","! cp /content/drive/'My Drive'/'EC 523 Project'/CompressedData/test.zip /content\n","! cp /content/drive/'My Drive'/'EC 523 Project'/CompressedData/val.zip /content\n","! cp /content/drive/'My Drive'/'EC 523 Project'/data/math.txt /content\n","\n","# from path will differ depending on where you saved the zip file in Google Drive\n","! unzip -DD -q  /content/train.zip -d  /content/\n","! unzip -DD -q  /content/test.zip -d  /content/\n","! unzip -DD -q  /content/val.zip -d  /content/\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":545,"status":"ok","timestamp":1701802022370,"user":{"displayName":"Robert D'Antonio","userId":"11761168232811440342"},"user_tz":300},"id":"cTBlEBQWM4rb","outputId":"cc8c42f4-c0e5-4602-cc2a-4399092d1d74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of train images: 158480\n","Number of test images: 30637\n","Number of validation images: 6765\n","Total images: 195882\n"]}],"source":["'''\n","Length of datasets\n","'''\n","num_train_str = !ls train | wc -l\n","num_test_str = !ls test | wc -l\n","num_val_str = !ls val | wc -l\n","num_train = int(num_train_str[0])\n","num_test = int(num_test_str[0])\n","num_val = int(num_val_str[0])\n","\n","print(f'Number of train images: {num_train}\\nNumber of test images: {num_test}\\nNumber of validation images: {num_val}\\nTotal images: {num_train+num_test+num_val}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6npeT0LetR9"},"outputs":[],"source":["train_root = \"/content/train/\"\n","test_root = \"/content/test/\"\n","val_root = \"/content/val/\"\n","label_file = \"/content/math.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13QS2mqeeSK8"},"outputs":[],"source":["import os\n","import cv2\n","import torch.utils.data\n","from PIL import Image\n","\n","\n","class LatexDataset(torch.utils.data.Dataset):\n","  def __init__(self, transform=None, dataroot=train_root): # can change dataroot to be either train_root, test_root, val_root\n","        '''Initialize the dataset.'''\n","        self.transform = transform\n","        self.dataroot = dataroot\n","        self.labels_txt = label_file\n","        self._parse()\n","\n","  def _parse(self):\n","        '''\n","        Parse the math.txt file.\n","        Populates the following private variables:\n","        - self.im_paths: A list of strings storing the associated image paths\n","        - self.labels: A list of strings, where each string is the latex code for an image\n","        '''\n","        def getImPath(idx):\n","            # Find image in either train, test, or validation folder\n","            imname = str(idx - 1).zfill(7) + '.png'\n","            if os.path.exists(f'{self.dataroot}{imname}'):\n","              impath = f'{self.dataroot}{imname}'\n","            else:\n","              return None\n","\n","            try:\n","                Image.open(impath).verify()\n","            except Exception as e:\n","                # Some images can't be opened\n","                # print(f\"Image at path {impath} is corrupted. Error: {e}\")\n","                return None\n","\n","            return impath\n","\n","        self.im_paths = []\n","        self.labels = []\n","\n","        with open(self.labels_txt) as f:\n","            for idx, line in enumerate(f):\n","                impath = getImPath(idx+1)\n","\n","                if impath is not None:\n","                    self.im_paths.append(impath)            # Image name\n","                    self.labels.append(line.strip('\\n'))    # String of latex code\n","\n","\n","  def __len__(self):\n","        '''Return length of the dataset.'''\n","        assert len(self.labels) == len(self.im_paths)\n","        return len(self.labels)\n","\n","  def __getitem__(self, index):\n","        '''\n","        Return the (image, attributes) tuple.\n","        This function gets called when you index the dataset.\n","        '''\n","        def img_load(index):\n","            imraw = Image.open(self.im_paths[index])\n","            imgray = imraw.convert('L')                         # Convert image to greyscale\n","            imthresh = imgray.point(lambda p: p > 240 and 255)  # Threshold image to remove background (white)\n","            im = self.transform(imthresh)\n","            return im\n","\n","        target = self.labels[index]\n","        return img_load(index), target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4mxXIoP5VpeE"},"outputs":[],"source":["'''\n","Dictionary block: converts a LaTeX string to a dictionary of latex tokens, where\n","each unique token has its own entry and integer value assigned to it\n","\n","'''\n","class LatexDict():\n","    def __init__(self, num_tokens=256):\n","        self.labels_txt = label_file\n","        self.num_tokens = num_tokens\n","        self.latex_dict = {'<UKN>':0, '<PAD>':1} # Initialize with token for unknown and for padding\n","        self.latex_dict_inverse = {0:'<UKN>', 1:'<PAD>'} # Initialize inverse dict for quicker reverse lookups\n","        self.create_dict()\n","\n","    def create_dict(self):\n","        # Go through entire label file and populate dictionary\n","        with open(self.labels_txt) as f:\n","            for line in f:\n","                tokens = line.split()\n","                for token in tokens:\n","                    if token not in self.latex_dict:\n","                        # Assign a new ID for the unseen token\n","                        new_id = len(self.latex_dict)\n","                        self.latex_dict[token] = new_id\n","                        self.latex_dict_inverse[new_id] = token\n","\n","    def map_tokens(self, tex_str_list, batch_size):\n","        ids_tensor = torch.full((batch_size, self.num_tokens), self.latex_dict['<PAD>'], dtype=torch.float32)\n","\n","        for row, tex_str in enumerate(tex_str_list):\n","            tokens = tex_str.split()\n","            for col, token in enumerate(tokens):\n","                ids_tensor[row, col] = self.latex_dict[token]\n","\n","        return ids_tensor\n","\n","    def tokens_to_tex(self, token_vec):\n","        tex_str = ' '\n","        for token_id in token_vec.tolist():\n","            if token_id in self.latex_dict_inverse:\n","                if self.latex_dict_inverse[token_id] != '<PAD>' and self.latex_dict_inverse[token_id] != '<UKN>':\n","                    tex_str += self.latex_dict_inverse[token_id] + ' '\n","\n","        return tex_str\n","\n","    def __dict__(self):\n","        return self.latex_dict\n","\n","    def __len__(self):\n","        return len(self.latex_dict)\n","\n","# latex_dict = LatexDict()\n"]},{"cell_type":"markdown","metadata":{"id":"GiPraE5GmoXL"},"source":["Creating the CNN Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDKuAzMJC6K4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import math\n","\n","class CNN_Block(nn.Module):\n","    def __init__(self, output_dims):\n","        super(CNN_Block, self).__init__()\n","        # self.conv1 = nn.Conv2d(3, 64, 3)\n","        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)    # Images are originally one channel, added padding as well\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        self.conv4 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.pool4 = nn.MaxPool2d(2, 2)\n","\n","        self.fc1 = nn.Linear(256 * 2 * 8, 1024)\n","        self.fc2 = nn.Linear(1024, output_dims)\n","\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","\n","        x = x.view(x.size(0),-1)   # Flatten so this can be used in linear layers\n","\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TN9Z5Sqdnwl"},"outputs":[],"source":["class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, batch_size):\n","        super(LSTM, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.batch_size = batch_size\n","\n","        self.cell_state = None\n","        self.hidden_state = None\n","\n","        #Input-gate parameters\n","        self.W_i = nn.Parameter(torch.zeros(self.num_layers, hidden_size, hidden_size + input_size, dtype=torch.double))\n","        self.b_i = nn.Parameter(torch.zeros(self.num_layers, hidden_size, 1, dtype=torch.double))\n","        #forget_gate parameters\n","        self.w_f = nn.Parameter(torch.zeros(self.num_layers, hidden_size, hidden_size + input_size, dtype=torch.double))\n","        self.b_f = nn.Parameter(torch.zeros(self.num_layers, hidden_size, 1, dtype=torch.double))\n","        #candidate parameters\n","        self.w_c = nn.Parameter(torch.zeros(self.num_layers, hidden_size, hidden_size + input_size, dtype=torch.double))\n","        self.b_c = nn.Parameter(torch.zeros(self.num_layers, hidden_size, 1, dtype=torch.double))\n","        #output gate parameters\n","        self.w_o = nn.Parameter(torch.zeros(self.num_layers, hidden_size, hidden_size + input_size, dtype=torch.double))\n","        self.b_o = nn.Parameter(torch.zeros(self.num_layers, hidden_size, 1, dtype=torch.double))\n","\n","        self.init_weights()\n","        self.reset_LSTM_states(batch_size)\n","\n","    def init_weights(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def reset_LSTM_states(self, batch_size):\n","        self.cell_state = torch.zeros(self.num_layers, batch_size, self.hidden_size, dtype=torch.double)\n","        self.hidden_state = torch.zeros(self.num_layers, batch_size, self.hidden_size, dtype=torch.double)\n","\n","    def forward(self, x):\n","        print(x.shape, self.hidden_state.shape)\n","\n","        X_H = torch.cat((x, self.hidden_state), dim=2)\n","        input_update = torch.sigmoid(torch.matmul(self.W_i, X_H) + self.b_i)\n","        forget_update = torch.sigmoid(torch.matmul(self.w_f, X_H) + self.b_f)\n","        candidate_update = torch.tanh(torch.matmul(self.w_c, X_H) + self.b_c)\n","        self.cell_state = forget_update * self.cell_state + input_update * candidate_update\n","        output_update = torch.sigmoid(torch.matmul(self.w_o, X_H) + self.b_o)\n","        self.hidden_state = output_update * torch.tanh(self.cell_state)\n","\n","        return self.hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lh3CqkeHZDr"},"outputs":[],"source":["class attention(nn.Module):\n","    def __init__(self, beta_size, hidden_size, v_length):\n","        super(attention, self).__init__()\n","        #weights for the hidden layer\n","        self.w_h = nn.Linear(hidden_size, beta_size, bias=False)\n","        #weights for the encoded image\n","        self.w_v = nn.Linear(v_length, beta_size, bias=False)\n","        #weights for the betas\n","        self.w_beta = nn.Parameter(torch.Tensor(beta_size))\n","        nn.init.uniform_(self.w_beta, -1e-2, 1e-2)\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        torch.nn.init.xavier_uniform_(self.w_h.weight)\n","        torch.nn.init.xavier_uniform_(self.w_v.weight)\n","\n","    def forward(self, V_new, h_t):\n","        #Multiplication\n","        U_t = torch.tanh(self.w_h(h_t).unsqueeze(1) + self.w_v(V_new)) # [B, H' * W', C]      !! Changed W to w_v\n","\n","        #activation + sum\n","        E_t = torch.sum(U_t * self.w_beta, dim=-1)\n","\n","        #activation\n","        A_t = torch.softmax(E_t, dim = 1).unsqueeze(1)\n","\n","        C_t = torch.matmul(A_t, V_new).squeeze(1)\n","\n","        return C_t, A_t"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5R7ji0yt1y0"},"outputs":[],"source":["def PositionalEmbedding2D(D_model,height,width):\n","  if D_model % 4 != 0:\n","        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n","                         \"odd dimension (got dim={:d})\".format(D_model))\n","  pe = torch.zeros(D_model,height,width)\n","  d_model = int(D_model / 2)\n","  div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n","  pos_w = torch.arange(0., width).unsqueeze(1)\n","  pos_h = torch.arange(0., height).unsqueeze(1)\n","  pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n","  pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n","  pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n","  pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n","\n","  return pe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8XBnnwT1S28X"},"outputs":[],"source":["'''\n","Overall Model Class\n","'''\n","\n","class Model(nn.Module):\n","    def __init__(self, max_tokens):\n","        super(Model, self).__init__()\n","        self.cnn = CNN_Block(output_dims=max_tokens)\n","        # self.lstm = LSTM(input_size=256, hidden_size=256, num_layers=1, batch_size=64)\n","        self.lstm = nn.LSTM(input_size=max_tokens, hidden_size=max_tokens, num_layers=1)\n","        self.attention = attention(beta_size=max_tokens, hidden_size=max_tokens, v_length=max_tokens)\n","        # self.attention = nn.MultiheadAttention(embed_dim=max_tokens, num_heads=4, batch_first=True)\n","        self.fc = nn.Linear(max_tokens, max_tokens)\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x, (h_n, c_n) = self.lstm(x)\n","        # x, _ = self.attention(x, x, x) # Broken !!\n","        x, h_t  = self.attention(x, h_n)\n","        # = self.fc(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16204,"status":"ok","timestamp":1701802916171,"user":{"displayName":"Robert D'Antonio","userId":"11761168232811440342"},"user_tz":300},"id":"ASkYo0volPTm","outputId":"8e3905b0-8e53-4112-cfa0-448dd5bd41a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda:0\n"]}],"source":["'''\n","Initialize dataset and image preprocessing\n","\n","NOTE:\n","    Some of the images in the dataset are corrupted. To deal with this,\n","    there is a check for each image to ensure that it can be loaded.\n","'''\n","import torchvision.transforms as transforms\n","\n","reduced_imsize = (32, 128)  # Images are reduced to this size\n","\n","# Define the transform pipeline - add normalization?\n","transform = transforms.Compose([\n","    transforms.Resize(reduced_imsize),\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = LatexDataset(transform=transform, dataroot=train_root)\n","test_dataset = LatexDataset(transform=transform, dataroot=test_root)\n","val_dataset = LatexDataset(transform=transform, dataroot=val_root)\n","\n","# Device settings\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f'Device: {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1576,"status":"ok","timestamp":1701802917742,"user":{"displayName":"Robert D'Antonio","userId":"11761168232811440342"},"user_tz":300},"id":"v1MFfod-mEzx","outputId":"81858eb6-92e5-4155-ff91-c2d153fade35"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["'''\n","Initialize hyperperameters, trainloader, and dictionary of LaTeX token mappings\n","'''\n","\n","# Hyperparameters\n","batch_size = 64\n","learning_rate = 0.001\n","# weight_decay = 0.00001  # (L2 penalty)\n","\n","max_tokens = 1784        # Maximum number of tokens in a latex string\n","\n","latex_dict = LatexDict(num_tokens=max_tokens)\n","\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8)\n","\n","# print(f'Dictionary length: {latex_dict.__len__()}')\n","# print(f'Dictionary: {latex_dict.__dict__()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbZ-jfJ8lY98"},"outputs":[],"source":["'''\n","Initialize Model\n","\n","Initialize Loss Functions:\n","1. Normal Cross-Entropy Loss between prediction and label\n","2. LaTeX compile test:\n","    - Custom function, returns True if code can compile into LaTeX, False if not\n","\n","Initialize Optimizer:\n","1. Adam Optimizer\n","'''\n","model = Model(max_tokens=max_tokens).to(device)\n","# criterion = nn.CrossEntropyLoss()    ## May need to change this\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g0eFWuLKm2e"},"outputs":[],"source":["# FOR DEBUG\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"KFMNCtWpqN7W","outputId":"67093ab0-3b3b-473b-d0c0-c8394a7e386f"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2474 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 1784])) that is different to the input size (torch.Size([1, 1784])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"," 40%|███▉      | 980/2474 [01:15<01:34, 15.88it/s]"]}],"source":["import random\n","from tqdm import tqdm\n","\n","\n","'''\n","Training Loop\n","'''\n","\n","num_epoch = 10\n","\n","for epoch in range(num_epoch):\n","    print('epoch:', epoch)\n","    pbar = tqdm(trainloader)\n","    for images, y in pbar:\n","\n","        images = images.to(device)              # Send to gpu\n","\n","        y_vec = latex_dict.map_tokens(y, batch_size=batch_size)\n","        y_vec = y_vec.to(device)                # Send to gpu\n","\n","        predictions = model(images)             # Get predictions\n","\n","        loss1 = criterion(predictions, y_vec)   # Calculate first loss function\n","\n","        # loss2 = can_compile(precitions)         # Check if output can compile\n","\n","        optimizer.zero_grad()\n","\n","        loss1.backward()\n","\n","        optimizer.step()\n","\n","    # After each epoch, print five of the output strings - throws cuda side assert error\n","    # random_indeces = [random.randint(1, batch_size) for _ in range(5)]\n","    # print(predictions[random_indeces])\n","    # token_ints = torch.floor(predictions).to(torch.int32)\n","    # for i, token_str in enumerate(token_ints[random_indeces]):\n","    #     print(f'prediction: {latex_dict.tokens_to_tex(token_str)}\\nlabel: {y_vec[i]}\\n\\n')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-bW8-I52Xue"},"outputs":[],"source":["# print(latex_dict.tokens_to_tex(predictions))\n","# import random\n","# print(predictions)\n","\n","# random_indeces = [random.randint(1, batch_size) for _ in range(5)]\n","# token_ints = torch.floor(predictions).to(torch.int32)\n","# for token_str in token_ints[random_indeces]:\n","#     print(latex_dict.tokens_to_tex(token_str))\n","\n","# print(token_ints[0])\n","# # print(token_ints[1])\n","# # print(token_ints[2])\n","# # print(token_ints[3])\n","# # print(token_ints[4])\n","\n","# a = latex_dict.tokens_to_tex(token_ints[0])\n","# print(a )"]},{"cell_type":"markdown","metadata":{"id":"_Z39xBOT8IPn"},"source":["####Test outputs here\n","\n","$$\n"," B \\lbrace r h q r \\rbrace \\alpha o \\alpha \\theta o \\theta q o c 1 q 2 2 1 1 . s 0 s \\} 0 = = t 4 ) 4 4 4 4 ) ) ( 3 \\, + \\{ \\{ \\{ \\prime \\frac + \\frac \\frac \\frac \\frac e \\frac \\prime \\mathrm \\prime \\mathrm \\zeta \\zeta \\mathrm e \\mathrm \\: d \\: \\: l d l \\infty l l \\infty d } ^ \\infty ^ ^ ^ \\infty ^ } ^ \\epsilon ^ } \\epsilon } \\epsilon \\epsilon } } \\epsilon - - - \\epsilon \\epsilon - \\epsilon - { - { - { - { - - { { { { { - { _ _ { { \\int - \\int _ { _ _ _ _ { _ _ _ _ _ \\int _ _ _ _ { \\int _ _ \\int _ _ _ \\int \\int \\int _ { _ \\int _ \\int \\int \\int _ \\int \\int \\int _ \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int _ \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int \\int\n","$$"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1J1aM8mQqdbUdab5sPAGLO9LyPOrDt50s","timestamp":1700463159650}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}